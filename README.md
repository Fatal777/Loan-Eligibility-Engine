# üè¶ Loan Eligibility Engine

An automated system that ingests user data, discovers personal loan products from public websites, matches users to eligible products, and notifies them via email

> üìñ **For comprehensive documentation**, see [README_DETAILED.md]

## üöÄ Live AWS Deployment

This project is fully deployed on AWS. Here are the live endpoints:

| Resource | URL/Endpoint |
|----------|--------------|
| **API Gateway** | `https://c41a2ucawd.execute-api.ap-south-1.amazonaws.com/dev` |
| **Health Check** | `GET /health` |
| **Get Upload URL** | `GET /upload/presigned-url` |
| **Trigger Matching** | `POST /trigger/matching` |
| **S3 Bucket** | `loan-eligibility-csv-uploads-202533497839` |
| **RDS PostgreSQL** | `loan-eligibility-db.chyeiw00stv1.ap-south-1.rds.amazonaws.com` |
| **n8n Dashboard** | `http://localhost:5678` (self-hosted) |
| **AWS Region** | `ap-south-1` (Mumbai) |

### Current Database Stats

| Metric | Value |
|--------|-------|
| Users Loaded | 10,000 |
| Loan Products | 8 (from 10 banks) |
| Matches Created | 25,610 |
| Batch ID | `20251206173422` |

### Quick Test Commands

```bash
# Health Check
curl https://c41a2ucawd.execute-api.ap-south-1.amazonaws.com/dev/health

# Get Presigned URL for Upload
curl "https://c41a2ucawd.execute-api.ap-south-1.amazonaws.com/dev/upload/presigned-url?filename=test.csv"

# Trigger Matching (POST)
curl -X POST https://c41a2ucawd.execute-api.ap-south-1.amazonaws.com/dev/trigger/matching \
  -H "Content-Type: application/json" \
  -d '{"batchId": "20251206173422"}'
```

---

## üìã Table of Contents

- [Architecture Overview](#architecture-overview)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Setup & Deployment](#setup--deployment)
- [n8n Workflow Configuration](#n8n-workflow-configuration)
- [Design Decisions](#design-decisions)
- [API Reference](#api-reference)
- [Testing](#testing)

---

## üîê Event-Driven Upload Pattern

We counter Lambda timeout and request size limits using a **professional event-driven pattern**:

1. **Presigned URL Generation** - Frontend requests a presigned URL from Lambda (lightweight, <1s)
2. **Direct S3 Upload** - Frontend uploads CSV directly to S3 (bypasses Lambda, no size limits)
3. **S3 Event Trigger** - S3 triggers processCSV Lambda automatically on file upload
4. **Async Processing** - Lambda processes CSV in background (no timeout constraints)

This pattern:
- ‚úÖ Eliminates Lambda 30-second timeout issues
- ‚úÖ Removes request payload size limits (10 MB)
- ‚úÖ Provides immediate user feedback
- ‚úÖ Scales to multi-GB files
- ‚úÖ Follows AWS best practices

See `src/handlers/upload.py` for implementation.

---

## üèóÔ∏è Architecture Overview

![Architecture Diagram](./Diagram.png)

### Data Flow

1. **CSV Upload**: User uploads CSV via frontend ‚Üí Presigned URL ‚Üí Direct S3 upload
2. **Processing**: S3 event triggers Lambda ‚Üí Parse CSV ‚Üí Store in PostgreSQL ‚Üí Trigger n8n webhook
3. **Crawling** (Daily): n8n crawls financial websites ‚Üí Extracts loan products ‚Üí Stores in database
4. **Matching**: n8n fetches users & products ‚Üí Multi-stage filtering ‚Üí Stores matches
5. **Notification**: n8n generates personalized emails ‚Üí Sends via AWS SES

---

## ‚ú® Features

- **Scalable CSV Upload**: Event-driven S3 upload pattern avoiding Lambda limits
- **Automated Web Crawling**: Daily discovery of loan products from financial websites
- **Intelligent Matching**: Multi-stage optimization pipeline for efficient user-loan matching
- **Email Notifications**: Personalized HTML emails via AWS SES
- **Self-Hosted Workflow Engine**: n8n for visual workflow automation

---

## üõ†Ô∏è Tech Stack

| Component | Technology |
|-----------|------------|
| Backend | Python 3.11, AWS Lambda |
| Database | Amazon RDS PostgreSQL |
| Storage | Amazon S3 |
| Email | Amazon SES |
| Workflow | n8n (self-hosted via Docker) |
| Infrastructure | Serverless Framework |
| Frontend | HTML, CSS, JavaScript (Vanilla) |
| AI (Optional) | Google Gemini / OpenAI GPT |

---

## üìÅ Project Structure

```
loan-eligibility-engine/
‚îú‚îÄ‚îÄ serverless.yml              # AWS infrastructure definition
‚îú‚îÄ‚îÄ docker-compose.yml          # n8n Docker setup
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ .env.example                # Environment variables template
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ handlers/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ upload.py           # Presigned URL generation
‚îÇ       ‚îú‚îÄ‚îÄ process_csv.py      # CSV processing & storage
‚îÇ       ‚îú‚îÄ‚îÄ trigger_matching.py # Manual workflow trigger
‚îÇ       ‚îî‚îÄ‚îÄ health.py           # Health check endpoint
‚îÇ
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îî‚îÄ‚îÄ init/
‚îÇ       ‚îî‚îÄ‚îÄ 01_schema.sql       # Database schema & sample data
‚îÇ
‚îú‚îÄ‚îÄ n8n/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ workflow_a_crawler.json      # Loan product discovery
‚îÇ       ‚îú‚îÄ‚îÄ workflow_b_matching.json     # User-loan matching
‚îÇ       ‚îî‚îÄ‚îÄ workflow_c_notification.json # Email notifications
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ index.html              # Upload UI
‚îÇ   ‚îú‚îÄ‚îÄ styles.css              # Styling
‚îÇ   ‚îî‚îÄ‚îÄ app.js                  # Frontend logic
‚îÇ
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ architecture.md         # Detailed architecture docs
```

---

## üì¶ Prerequisites

1. **AWS Account** with Free Tier access
2. **AWS CLI** configured with credentials
3. **Node.js** v18+ and npm
4. **Docker** and Docker Compose
5. **Python** 3.11+
6. **Serverless Framework** (`npm install -g serverless`)

---

## üöÄ Setup & Deployment

### Step 1: Clone Repository

```bash
git clone https://github.com/Fatal777/Loan-Eligibility-Engine.git
cd Loan-Eligibility-Engine
```

### Step 2: Set Up Python Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows (PowerShell):
.\venv\Scripts\Activate.ps1

# On Windows (Command Prompt):
venv\Scripts\activate.bat

# On macOS/Linux:
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt
```

### Step 3: Configure Environment

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your values
# - AWS credentials
# - RDS database credentials
# - n8n configuration
# - AI API keys (optional)
```

### Step 4: Set Up Amazon RDS PostgreSQL

1. Create RDS PostgreSQL instance via AWS Console
2. Configure security group to allow n8n and Lambda access
3. Note the endpoint, username, and password
4. Connect and run the schema:

```bash
psql -h your-rds-endpoint -U your-username -d loan_eligibility -f database/init/01_schema.sql
```

### Step 5: Deploy AWS Infrastructure

```bash
# Install Serverless plugins
npm install

# Deploy to AWS
serverless deploy --stage dev

# Note the API Gateway URL from output
```

### Step 6: Launch n8n

```bash
# Start n8n with Docker Compose
docker-compose up -d

# Access n8n at http://localhost:5678
# Default credentials: admin / admin123 (change in .env)
```

### Step 7: Configure n8n

1. **Add PostgreSQL Credentials**:
   - Go to Settings ‚Üí Credentials ‚Üí Add Credential
   - Select "Postgres"
   - Enter your RDS connection details
   - Name it "Loan Eligibility DB"

2. **Add AWS Credentials**:
   - Add credential for "AWS"
   - Enter Access Key ID and Secret Key
   - Select region (us-east-1)

3. **Import Workflows**:
   - Go to Workflows ‚Üí Import from File
   - Import all three workflow JSON files from `n8n/workflows/`

4. **Activate Workflows**:
   - Open each workflow
   - Click "Active" toggle
   - Save

### Step 8: Configure Frontend

```javascript
// Edit frontend/app.js
const CONFIG = {
    API_BASE_URL: 'https://your-api-gateway-url.execute-api.us-east-1.amazonaws.com/dev'
};
```

### Step 9: Verify AWS SES

```bash
# Verify sender email (required in sandbox mode)
aws ses verify-email-identity --email-address noreply@yourdomain.com

# For testing, also verify recipient emails
aws ses verify-email-identity --email-address test@example.com
```

---

## ‚öôÔ∏è n8n Workflow Configuration

### Workflow A: Loan Product Discovery (Web Crawler)

| Setting | Value |
|---------|-------|
| Trigger | Schedule (Daily at 00:00 UTC) + Manual |
| Websites | BankBazaar, PaisaBazaar, MyLoanCare |
| Output | loan_products table |

**Features**:
- Real web crawling from loan aggregator sites
- Extracts: product name, provider, interest rates, loan amounts, eligibility criteria
- Robust error handling (continues if one site fails)
- Deduplication with UPSERT

### Workflow B: User-Loan Matching (Scalable)

| Setting | Value |
|---------|-------|
| Trigger | Webhook (POST /webhook/trigger-matching) + Manual |
| Batch Size | 500 users per iteration |
| Output | matches table |

**Scalability Features**:
- Self-looping: automatically processes next batch until done
- SQL-based matching (not n8n loops) for performance
- Race-safe with `FOR UPDATE SKIP LOCKED`
- Auto-triggers Workflow C when complete

### Workflow C: User Notification (Production Scale)

| Setting | Value |
|---------|-------|
| Trigger | Webhook (POST /webhook/trigger-notification) + Manual |
| Email Service | AWS SES (ap-south-1) |
| Batch Size | 1000 users per iteration |
| Template | Personalized HTML with loan details |

**Features**:
- Bulk fetch with SQL aggregation (products grouped per user)
- Beautiful HTML email template with match scores
- Self-looping for unlimited scale
- Tracks notification status in database

---

## üí° Design Decisions

### 1. Event-Driven CSV Upload

**Problem**: Direct Lambda upload hits size limits (6MB payload) and timeout issues.

**Solution**: Presigned URL pattern
- Frontend requests presigned URL from Lambda
- Frontend uploads directly to S3
- S3 event triggers processing Lambda

**Benefits**:
- Supports files up to 5GB
- No Lambda timeout issues
- Reduced costs (no Lambda for upload)

### 2. Optimization Treasure Hunt Solution

**Problem**: Matching thousands of users against dozens of products is O(n√óm) complexity. Using LLM for every pair is slow and expensive.

**Solution**: Multi-Stage Filtering Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  OPTIMIZATION PIPELINE                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                   ‚îÇ
‚îÇ  Stage 1: SQL Pre-Filter                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Only fetch active products                             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Only fetch unprocessed users from current batch        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Reduces dataset before n8n processing                  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                           ‚Üì                                      ‚îÇ
‚îÇ  Stage 2: Index-Based Bucketing                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Group products by credit score ranges:                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   [300-500] [500-650] [650-750] [750-900]               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ User only compared against relevant bucket             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Reduces comparisons by ~75%                            ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                           ‚Üì                                      ‚îÇ
‚îÇ  Stage 3: Rule-Based Matching                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Fast boolean eligibility checks:                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   ‚úì Credit score in range                                ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   ‚úì Monthly income ‚â• minimum                             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   ‚úì Age in range                                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   ‚úì Employment status matches                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Calculate confidence score (0-100)                     ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                           ‚Üì                                      ‚îÇ
‚îÇ  Stage 4: LLM Verification (Optional)                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Only for matches with score 50-70 (edge cases)         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Typically <10% of matches need LLM review              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Evaluates qualitative factors                          ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Results**:
- 10,000 users √ó 50 products = 500,000 comparisons (naive)
- With bucketing: ~125,000 comparisons
- LLM calls: ~12,500 (only edge cases)
- **Cost reduction: 75%+ in compute, 97.5% in LLM calls**

### 3. Web Crawling Strategy

**Approach**:
- Target loan aggregator sites (BankBazaar, PaisaBazaar) for broad coverage
- Also crawl individual bank sites for accuracy
- Use regex patterns for basic extraction
- Optional AI-powered extraction for complex layouts

**Robustness**:
- Continue on failure (one site down doesn't stop workflow)
- Store raw criteria for audit trail
- Last updated timestamp for freshness tracking

---

## üì° API Reference

### Local Development Setup (Docker)

For local testing without AWS:

```bash
# Start PostgreSQL + n8n containers
docker-compose up -d

# Start local Flask API
python local_api.py

# Access points:
# - Local API: http://localhost:3000
# - n8n: http://localhost:5678
# - PostgreSQL: localhost:5432
```

**Verified Email Configuration (AWS SES)**:
- Sender: `saadilkal.10@gmail.com`
- Region: `ap-south-1` (Mumbai)

### GET /upload/presigned-url

Get a presigned URL for S3 upload.

**Query Parameters**:
- `filename` (optional): Original filename for metadata

**Response**:
```json
{
  "success": true,
  "presignedUrl": "https://bucket.s3.amazonaws.com/...",
  "uploadId": "uuid",
  "key": "uploads/timestamp_uuid.csv",
  "expiresIn": 3600
}
```

### POST /trigger/matching

Manually trigger matching workflow.

**Body**:
```json
{
  "batchId": "20241205120000"  // optional, defaults to latest
}
```

### GET /health

Health check endpoint.

**Response**:
```json
{
  "status": "healthy",
  "service": "loan-eligibility-engine",
  "version": "1.0.0"
}
```

---

## üß™ Testing

### Test CSV Upload

1. Open `frontend/index.html` in browser
2. Upload the sample `users.csv` file
3. Check S3 bucket for uploaded file
4. Check RDS for stored users
5. Verify n8n workflow execution

### Test Matching Workflow

```bash
# Trigger manually via API
curl -X POST https://your-api/dev/trigger/matching \
  -H "Content-Type: application/json" \
  -d '{"batchId": "your-batch-id"}'
```

### Test Email Notification

1. Ensure SES is configured
2. Add test email to verified identities
3. Run matching workflow
4. Check inbox for notification

---

## üìÑ License

MIT License - See LICENSE file for details.

---

## üë• Contributors

- [Saad Ilkal](https://github.com/Fatal777)

---

## üìß Contact

For questions or issues, please open a GitHub issue or contact:
- saadilkal.10@gmail.com
